@article{Kleer1986AnAT,
  title={An Assumption-Based TMS},
  author={J. D. Kleer},
  journal={Artif. Intell.},
  year={1986},
  volume={28},
  pages={127-162}
}

@incollection{hansen2000probabilistic,
  title={Probabilistic satisfiability},
  author={Hansen, Pierre and Jaumard, Brigitte},
  booktitle={Handbook of Defeasible Reasoning and Uncertainty Management Systems},
  pages={321--367},
  year={2000},
  publisher={Springer}
}

@article{andersen2001easy,
  title={Easy cases of probabilistic satisfiability},
  author={Andersen, Kim Allan and Pretolani, Daniele},
  journal={Annals of Mathematics and Artificial Intelligence},
  volume={33},
  number={1},
  pages={69--91},
  year={2001},
  publisher={Springer}
}

@inproceedings{ribeiro-etal-2020-beyond,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.442",
    doi = "10.18653/v1/2020.acl-main.442",
    pages = "4902--4912",
}

@inproceedings{li-etal-2019-logic,
    title = "A Logic-Driven Framework for Consistency of Neural Models",
    author = "Li, Tao  and
      Gupta, Vivek  and
      Mehta, Maitrey  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1405",
    doi = "10.18653/v1/D19-1405",
    pages = "3924--3935",
    abstract = "While neural models show remarkable accuracy on individual predictions, their internal beliefs can be inconsistent across examples. In this paper, we formalize such inconsistency as a generalization of prediction error. We propose a learning framework for constraining models using logic rules to regularize them away from inconsistency. Our framework can leverage both labeled and unlabeled examples and is directly compatible with off-the-shelf learning schemes without model redesign. We instantiate our framework on natural language inference, where experiments show that enforcing invariants stated in logic can help make the predictions of neural models both accurate and consistent.",
}

@inproceedings{du2019consistent,
  title={Be Consistent! Improving Procedural Text Comprehension using Label Consistency},
  author={Du, Xinya and Dalvi, Bhavana and Tandon, Niket and Bosselut, Antoine and Yih, Wen-tau and Clark, Peter and Cardie, Claire},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2347--2356},
  year={2019}
}

@article{Mishra2017DomainTargetedHP,
  title={Domain-Targeted, High Precision Knowledge Extraction},
  author={B. D. Mishra and Niket Tandon and P. Clark},
  journal={Transactions of the Association for Computational Linguistics},
  year={2017},
  volume={5},
  pages={233-246}
}

@inproceedings{Thimm:2009d,
	Author = {Matthias Thimm},
	Booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI'09)},
	Editor = {Jeff Bilmes and Andrew Ng},
	Month = {June},
	Pages = {530--537},
	Location = {Montreal, Canada},
	Publisher = {AUAI Press},
	Title = {{Measuring Inconsistency in Probabilistic Knowledge Bases}},
	Year = {2009}
}
@inproceedings{Pujara2013KnowledgeGI,
  title={Knowledge Graph Identification},
  author={J. Pujara and H. Miao and L. Getoor and William W. Cohen},
  booktitle={International Semantic Web Conference},
  year={2013}
}

@inproceedings{Broecheler2010ProbabilisticSL,
  title={Probabilistic Similarity Logic},
  author={Matthias Broecheler and Lilyana Mihalkova and L. Getoor},
  booktitle={UAI},
  year={2010}
}

@article{muino2011measuring,
  title={Measuring and repairing inconsistency in probabilistic knowledge bases},
  author={Mui{\~n}o, David Picado},
  journal={International Journal of Approximate Reasoning},
  volume={52},
  number={6},
  pages={828--840},
  year={2011},
  publisher={Elsevier}
}

@article{Thimm:2013,
	Author = {Matthias Thimm},
	Journal = {Artificial Intelligence},
	Month = {April},
	Pages = {1--24},
	Title = {Inconsistency Measures for Probabilistic Logics},
	Volume = {197},
	Year = {2013}
}

@inproceedings{Petroni2019LanguageMA,
  title={Language Models as Knowledge Bases?},
  author={F. Petroni and Tim Rockt{\"a}schel and Patrick Lewis and A. Bakhtin and Yuxiang Wu and Alexander H. Miller and S. Riedel},
  booktitle={EMNLP},
  year={2019}
}

@article{Thorne2020NeuralD,
  title={Neural Databases},
  author={James Thorne and Majid Yazdani and Marzieh Saeidi and F. Silvestri and Sebastian Riedel and A. Halevy},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.06973}
}

@inproceedings{kassner-etal-2020-pretrained,
    title = "Are Pretrained Language Models Symbolic Reasoners over Knowledge?",
    author = {Kassner, Nora  and
      Krojer, Benno  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.conll-1.45",
    doi = "10.18653/v1/2020.conll-1.45",
    pages = "552--564",
    abstract = "How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success.",
}

@inproceedings{roberts-etal-2020-much,
    title = "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
    author = "Roberts, Adam  and
      Raffel, Colin  and
      Shazeer, Noam",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.437",
    doi = "10.18653/v1/2020.emnlp-main.437",
    pages = "5418--5426",
    abstract = "It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.",
}

@article{Talmor2020TeachingPM,
  title={Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge},
  author={Alon Talmor and Oyvind Tafjord and P. Clark and Yoav Goldberg and Jonathan Berant},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.06609}
}

@article{Elazar2021MeasuringAI,
  title={Measuring and Improving Consistency in Pretrained Language Models},
  author={Yanai Elazar and Nora Kassner and Shauli Ravfogel and Abhilasha Ravichander and E. Hovy and H. Schutze and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.01017}
}

@inproceedings{Camburu2020MakeUY,
  title={Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations},
  author={Oana-Maria Camburu and Brendan Shillingford and Pasquale Minervini and Thomas Lukasiewicz and P. Blunsom},
  booktitle={ACL},
  year={2020}
}

@inproceedings{ravichander-etal-2020-systematicity,
    title = "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in {BERT}",
    author = "Ravichander, Abhilasha  and
      Hovy, Eduard  and
      Suleman, Kaheer  and
      Trischler, Adam  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.starsem-1.10",
    pages = "88--102",
    abstract = "Contextualized word representations have become a driving force in NLP, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question: \textit{do probing studies shed light on systematic knowledge in BERT representations?} As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT {`}understands{'} a concept, and it cannot be expected to systematically generalize across applicable contexts.",
}

@incollection{wordnet,
  author      = "Fellbaum, Christiane",
  title       = "WordNet and wordnets",
  editor      = "Brown, Keith et al.",
  booktitle   = "Encyclopedia of Language and Linguistics, Second Edition",
  publisher   = "Elsevier",
  address     = "Oxford",
  year        = 2005
}

@inproceedings{Speer2017ConceptNet5A,
  title={ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
  author={Robyn Speer and Joshua Chin and Catherine Havasi},
  booktitle={AAAI},
  year={2017}
}


@inproceedings{subramanian-etal-2020-obtaining,
    title = "Obtaining Faithful Interpretations from Compositional Neural Networks",
    author = "Subramanian, Sanjay  and
      Bogin, Ben  and
      Gupta, Nitish  and
      Wolfson, Tomer  and
      Singh, Sameer  and
      Berant, Jonathan  and
      Gardner, Matt",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.495",
    doi = "10.18653/v1/2020.acl-main.495",
    pages = "5594--5608",
    abstract = "Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model{'}s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",
}



@inproceedings{Minervini2018AdversariallyRN,
  title={Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge},
  author={Pasquale Minervini and S. Riedel},
  booktitle={CoNLL},
  year={2018}
}

@inproceedings{Li2019ALF,
  title={A Logic-Driven Framework for Consistency of Neural Models},
  author={Tao Li and Vivek Gupta and Maitrey Mehta and Vivek Srikumar},
  booktitle = {EMNLP},
  year={2019}
}

@inproceedings{Ribeiro2019AreRR,
  title={Are Red Roses Red? Evaluating Consistency of Question-Answering Models},
  author={Marco Tulio Ribeiro and Carlos Guestrin and Sameer Singh},
  booktitle={ACL},
  year={2019}
}

@inproceedings{Tandon2018ReasoningAA,
  title={Reasoning about Actions and State Changes by Injecting Commonsense Knowledge},
  author={Niket Tandon and Bhavana Dalvi and Joel Grus and Wen-tau Yih and Antoine Bosselut and Peter Clark},
  booktitle={EMNLP},
  year={2018}
}

@inproceedings{Du2019BeCI,
  title={Be Consistent! Improving Procedural Text Comprehension using Label Consistency},
  author={X. Du and Bhavana Dalvi Mishra and Niket Tandon and Antoine Bosselut and Wen-tau Yih and P. Clark and Claire Cardie},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{berant2010global,
  title={Global learning of focused entailment graphs},
  author={Berant, Jonathan and Dagan, Ido and Goldberger, Jacob},
  booktitle={ACL},
  pages={1220--1229},
  year={2010}
}

@article{Graves2016HybridCU,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={A. Graves and Greg Wayne and M. Reynolds and Tim Harley and Ivo Danihelka and Agnieszka Grabska-Barwinska and Sergio Gomez Colmenarejo and Edward Grefenstette and Tiago Ramalho and J. Agapiou and Adri{\`a} Puigdom{\`e}nech Badia and K. Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and C. Summerfield and P. Blunsom and K. Kavukcuoglu and D. Hassabis},
  journal={Nature},
  year={2016},
  volume={538},
  pages={471-476}
}

@article{Asai2020LogicGuidedDA,
  title={Logic-Guided Data Augmentation and Regularization for Consistent Question Answering},
  author={Akari Asai and Hannaneh Hajishirzi},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.10157}
}

@article{ettinger-2020-bert,
    title = "What {BERT} Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
    author = "Ettinger, Allyson",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.3",
    doi = "10.1162/tacl_a_00298",
    pages = "34--48",
    abstract = "Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction{---} and, in particular, it shows clear insensitivity to the contextual impacts of negation.",
}

@inproceedings{kassner-schutze-2020-negated,
    title = "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
    author = {Kassner, Nora  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.698",
    doi = "10.18653/v1/2020.acl-main.698",
    pages = "7811--7818",
    abstract = "Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated ({`}{`}Birds cannot [MASK]{''}) and non-negated ({`}{`}Birds can [MASK]{''}) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add {``}misprimes{''} to cloze questions ({`}{`}Talk? Birds can [MASK]{''}). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
}
@inproceedings{Henaff2017TrackingTW,
  title={Tracking the World State with Recurrent Entity Networks},
  author={Mikael Henaff and J. Weston and Arthur D. Szlam and Antoine Bordes and Y. LeCun},
  booktitle = {ICLR},
  year={2016}
}

@inproceedings{Sukhbaatar2015EndToEndMN,
  title={End-To-End Memory Networks},
  author={Sainbayar Sukhbaatar and Arthur D. Szlam and J. Weston and R. Fergus},
  booktitle={NIPS},
  year={2015}
}

@article{arc-da,
  title={Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the Direct-Answer AI2 Reasoning Challenge},
  author={Sumithra Bhakthavatsalam and Daniel Khashabi and Tushar Khot and B. D. Mishra and Kyle Richardson and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord and P. Clark},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.03315}
}

@inproceedings{Petroni2019LanguageMA,
  title={Language Models as Knowledge Bases?},
  author={F. Petroni and Tim Rockt{\"a}schel and Patrick Lewis and A. Bakhtin and Yuxiang Wu and Alexander H. Miller and S. Riedel},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{Kassner2020NegatedAM,
  title={Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly},
  author={Nora Kassner and H. Schutze},
  booktitle={ACL},
  year={2020}
}

@article{Elazar2021MeasuringAI,
  title={Measuring and Improving Consistency in Pretrained Language Models},
  author={Yanai Elazar and Nora Kassner and Shauli Ravfogel and Abhilasha Ravichander and E. Hovy and H. Schutze and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.01017}
}

@inproceedings{Seo2017BidirectionalAF,
  title={Bidirectional Attention Flow for Machine Comprehension},
  author={Minjoon Seo and Aniruddha Kembhavi and Ali Farhadi and Hannaneh Hajishirzi},
  booktitle={ICLR},
  year={2016}
}

@inproceedings{JohnsonLaird1983MentalM,
  title={Mental Models : Towards a Cognitive Science of Language},
  author={P. Johnson-Laird},
  year={1983}
}